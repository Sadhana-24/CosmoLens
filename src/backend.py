import requests
import json
import os
import time
import traceback
from fastapi import FastAPI, HTTPException, Body, Form, UploadFile, File
from pydantic import BaseModel, Field
from dotenv import load_dotenv # To load .env file
from typing import List, Optional, Dict, Any
from fastapi.middleware.cors import CORSMiddleware
import base64
from io import BytesIO

# --- Load Environment Variables ---
load_dotenv()

# --- Configuration ---
# Load from environment or use placeholders (update placeholders if not using .env)
JINA_API_URL = 'https://api.jina.ai/v1/embeddings'
JINA_MODEL = "jina-embeddings-v3"
JINA_API_KEY = os.getenv("JINA_API_KEY", "YOUR_JINA_API_KEY")

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY", "YOUR_PINECONE_API_KEY")
PINECONE_INDEX_HOST = os.getenv("PINECONE_INDEX_HOST", "YOUR_INDEX_HOST")

# Multiple namespaces configuration
PINECONE_NAMESPACES = [
    "clip-embeddings",
    "astrollava-embeddings",
    "hubble-embeddings"
]
PINECONE_TOP_K_PER_NAMESPACE = 10  # Top K for each namespace

# Path to JSON file containing ID to URL mappings
ID_TO_URL_MAP_PATH = os.getenv("ID_TO_URL_MAP_PATH", "/Volumes/Sadhana/PESU SEM 6/genai proj/combined_astrollava_embeddings.jsonl")

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "YOUR_GEMINI_API_KEY")
GEMINI_MODEL = "gemini-2.0-flash" # Using Flash for potentially faster responses

# Pinecone Client (Initialize later when needed or globally if preferred)
from pinecone import Pinecone
try:
    # Try importing the specific exception class if available
    from pinecone.exceptions import ApiException
except ImportError:
    # Fallback to general Exception if the specific class isn't found
    print("Warning: Could not import pinecone.exceptions.ApiException. Using general Exception for Pinecone errors.")
    ApiException = Exception # Use general Exception as a fallback

# Gemini Client (Import the correct package)
import google.genai as genai
from google.genai import types

# --- Pydantic Models for API ---
class QueryRequest(BaseModel):
    query: str = Field(..., min_length=1, description="The user's query about astronomy/Hubble.")

class ContextItem(BaseModel):
    text: str
    namespace: str
    id: Optional[str] = None
    url: Optional[str] = None

class QueryResponse(BaseModel):
    answer: str = Field(..., description="The RAG-generated answer.")
    retrieved_context: List[ContextItem] = Field(default=[], description="List of context snippets retrieved from Pinecone.")
    error: Optional[str] = Field(default=None, description="Error message if processing failed.")

# --- Dummy Models for Multimodal ---
class MultimodalQueryRequest(BaseModel):
    text: Optional[str] = Field(default=None, description="Optional text part of the query.")

class MultimodalResponse(BaseModel):
    text_response: Optional[str] = Field(default=None, description="Generated text response.")
    image_url: Optional[str] = Field(default=None, description="URL of a generated image (dummy).")
    content_urls: Optional[List[str]] = Field(default=None, description="URLs of retrieved content.")
    message: str = Field(..., description="Status message.")

# --- Initialize FastAPI App ---
app = FastAPI(
    title="Astronomy RAG API",
    description="Provides answers to astronomy questions using Hubble data retrieved via vector search and generated by Gemini.",
    version="1.0.0",
)

origins = ["*"] # Allow all origins

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Helper Functions ---
# Load ID to URL mapping
id_to_url_map = {}

def load_id_to_url_map():
    """Load the mapping of IDs to URLs from a JSONL file."""
    global id_to_url_map
    try:
        if os.path.exists(ID_TO_URL_MAP_PATH):
            # Read JSONL file line by line
            with open(ID_TO_URL_MAP_PATH, 'r') as f:
                line_count = 0
                for line in f:
                    try:
                        # Parse each line as a separate JSON object
                        data = json.loads(line)
                        if 'id' in data and 'url' in data:
                            id_to_url_map[data['id']] = data['url']
                        elif 'metadata' in data and 'id' in data['metadata'] and 'url' in data['metadata']:
                            # Handle nested metadata structure
                            id_to_url_map[data['metadata']['id']] = data['metadata']['url']
                        line_count += 1
                    except json.JSONDecodeError as e:
                        print(f"Error parsing line {line_count} in JSONL file: {e}")
                        continue
                
            print(f"Loaded {len(id_to_url_map)} ID-URL mappings from JSONL file.")
        else:
            print(f"Warning: ID-URL mapping file not found at {ID_TO_URL_MAP_PATH}")
    except Exception as e:
        print(f"Error loading ID-URL mapping: {e}")
        traceback.print_exc()

# Load the mapping at startup
load_id_to_url_map()

async def get_jina_embedding_for_query(query_text: str) -> Optional[List[float]]:
    """Gets a single embedding vector for the user query using Jina."""
    if not JINA_API_KEY or JINA_API_KEY == "YOUR_JINA_API_KEY":
        print("Error: Jina API key not configured.")
        raise HTTPException(status_code=500, detail="Jina API key not configured.")

    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {JINA_API_KEY}'}
    data = {"model": JINA_MODEL, "input": [query_text]}

    try:
        # Using requests synchronously (FastAPI handles threading)
        response = requests.post(JINA_API_URL, headers=headers, data=json.dumps(data), timeout=60)
        response.raise_for_status()
        response_data = response.json()

        # Check response structure (Jina v3)
        if isinstance(response_data, list) and len(response_data) > 0 and isinstance(response_data[0], dict) and 'embedding' in response_data[0]:
            return response_data[0]['embedding']
        elif isinstance(response_data, dict) and 'data' in response_data and isinstance(response_data['data'], list) and len(response_data['data']) > 0 and isinstance(response_data['data'][0], dict) and 'embedding' in response_data['data'][0]:
             return response_data['data'][0]['embedding']
        else:
            print(f"Error: Unexpected response format from Jina API: {response_data}")
            raise HTTPException(status_code=500, detail="Unexpected response format from Jina embedding API.")

    except requests.exceptions.RequestException as e:
        status_code = 500
        detail = f"Error calling Jina API: {e}"
        if hasattr(e, 'response') and e.response is not None:
            status_code = e.response.status_code
            try: detail = f"Jina API Error (Status {status_code}): {e.response.json()}"
            except json.JSONDecodeError: detail = f"Jina API Error (Status {status_code}): {e.response.text}"
        print(detail)
        # Return 503 Service Unavailable for rate limits, 500 otherwise
        raise HTTPException(status_code=503 if status_code == 429 else 500, detail=detail)
    except Exception as e:
        print(f"An unexpected error occurred during Jina embedding: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Internal server error during embedding: {e}")

async def search_pinecone_multiple_namespaces(query_vector: List[float], top_k_per_namespace: int, index_host: str, namespaces: List[str]) -> List[ContextItem]:
    """Queries Pinecone index across multiple namespaces and returns relevant context snippets with IDs and URLs."""
    if not PINECONE_API_KEY or PINECONE_API_KEY == "YOUR_PINECONE_API_KEY":
        raise HTTPException(status_code=500, detail="Pinecone API key not configured.")
    if not index_host or index_host == "YOUR_INDEX_HOST":
        raise HTTPException(status_code=500, detail="Pinecone index host not configured.")

    all_retrieved_contexts = []
    
    try:
        # Initialize connection
        pc = Pinecone(api_key=PINECONE_API_KEY)
        index = pc.Index(host=index_host)
        
        # Query each namespace separately
        for namespace in namespaces:
            print(f"Querying namespace: {namespace}")
            try:
                query_response = index.query(
                    namespace=namespace,
                    vector=query_vector,
                    top_k=top_k_per_namespace,
                    include_metadata=True,
                    include_values=False
                )
                
                for match in query_response.matches:
                    metadata = match.metadata
                    if metadata:
                        # Prioritize fields likely containing the core text
                        context = metadata.get("caption", metadata.get("combined_text_preview", metadata.get("text", "")))
                        if context and isinstance(context, str):  # Ensure context is a non-empty string
                            # Create a context item with the match ID
                            context_item = ContextItem(
                                text=context,
                                namespace=namespace,
                                id=match.id
                            )
                            
                            # Add URL for astrollava-embeddings namespace
                            if namespace == "astrollava-embeddings" and match.id in id_to_url_map:
                                context_item.url = id_to_url_map.get(match.id)
                            
                            all_retrieved_contexts.append(context_item)
                        else:
                            print(f"Warning: Match ID {match.id} in namespace {namespace} missing/invalid context field in metadata: {metadata}")
                    else:
                        print(f"Warning: Match ID {match.id} in namespace {namespace} has no metadata.")
                
                print(f"Retrieved {len(query_response.matches)} contexts from namespace {namespace}")
                
            except Exception as e:
                print(f"Error querying namespace {namespace}: {e}")
                # Continue with other namespaces even if one fails
                continue

    except ApiException as e:
        print(f"Error connecting to Pinecone: {e}")
        raise HTTPException(status_code=503, detail=f"Pinecone query failed: {e}")
    except Exception as e:
        print(f"An unexpected error occurred during Pinecone search: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Internal server error during vector search: {e}")

    return all_retrieved_contexts

async def generate_response_with_gemini(query: str, context_items: List[ContextItem]) -> str:
    """Generates a response using Gemini based on the query and retrieved context."""
    if not GEMINI_API_KEY or GEMINI_API_KEY == "YOUR_GEMINI_API_KEY":
        raise HTTPException(status_code=500, detail="Gemini API key not configured.")

    try:
        # Configure the Gemini client
        client = genai.Client(api_key=GEMINI_API_KEY)

        # Convert ContextItems to text format for prompt
        context_texts = []
        for i, item in enumerate(context_items):
            context_str = f"Context {i+1} [{item.namespace}]: {item.text}"
            if item.url:
                context_str += f" (Source: {item.url})"
            context_texts.append(context_str)
            
        # Combine retrieved contexts into a single string block
        context_str = "\n\n".join(context_texts)
        if not context_items:
            context_str = "No relevant context was found."

        # Create a clear prompt for the RAG task
        prompt = f"""You are a helpful assistant knowledgeable about astronomy, specifically the Hubble Space Telescope findings. Answer the following user query based *only* on the provided context information. If the context doesn't contain the answer, state that clearly. Do not use any external knowledge.

Context Information:
---
{context_str}
---

User Query: {query}

Answer:"""

        # Prepare the contents for the Gemini API
        contents = [
            types.Content(
                role="user",
                parts=[types.Part.from_text(text=prompt)],
            )
        ]

        # Generate content using the Gemini API
        generate_content_config = types.GenerateContentConfig(response_mime_type="text/plain")

        response_text = ""
        # Use a standard 'for' loop as generate_content_stream returns a sync iterator
        for chunk in client.models.generate_content_stream(
            model=GEMINI_MODEL,
            contents=contents,
            config=generate_content_config,
        ):
            response_text += chunk.text

        return response_text.strip()

    except Exception as e:
        print(f"An error occurred during Gemini generation: {e}")
        traceback.print_exc()
        # Ensure the original error type isn't lost if it's already an HTTPException
        if isinstance(e, HTTPException):
             raise e
        # Wrap other exceptions in a 503 Service Unavailable
        raise HTTPException(status_code=503, detail=f"Error generating response from Gemini: {e}")

# Step 6: Generate final response using Gemini with both text and image
async def generate_response_with_gemini_multimodal(query: str, context_items: List[ContextItem], image_bytes=None, mime_type=None) -> str:
    """Generates a response using Gemini based on query, retrieved context, and an optional image."""
    if not GEMINI_API_KEY or GEMINI_API_KEY == "YOUR_GEMINI_API_KEY":
        raise HTTPException(status_code=500, detail="Gemini API key not configured.")

    try:
        # Configure the Gemini client
        client = genai.Client(api_key=GEMINI_API_KEY)

        # Convert ContextItems to text format for prompt
        context_texts = []
        for i, item in enumerate(context_items):
            context_str = f"Context {i+1} [{item.namespace}]: {item.text}"
            if item.url:
                context_str += f" (Source: {item.url})"
            context_texts.append(context_str)
            
        # Combine retrieved contexts into a single string block
        context_str = "\n\n".join(context_texts)
        if not context_items:
            context_str = "No relevant context was found."

        # Create a clear prompt for the RAG task
        prompt = f"""You are a helpful assistant knowledgeable about astronomy, specifically the Hubble Space Telescope findings. 
Answer the following user query based on the provided context information and the image (if any).
If the context doesn't contain the answer, use your knowledge about astronomy to complement, but make it clear what information comes from the context.

Context Information:
---
{context_str}
---

User Query: {query}

Answer with detailed information about what's shown in the image and relate it to the user's query:"""

        parts = []
        
        # Add the image first if we have one
        if image_bytes and mime_type:
            # Base64 encode the image
            image_base64 = base64.b64encode(image_bytes).decode('utf-8')
            parts.append({
                "inline_data": {
                    "mime_type": mime_type,
                    "data": image_base64
                }
            })
            
        # Then add the text prompt
        parts.append({"text": prompt})
        
        # Create multimodal content with both image and text
        contents = [{"role": "user", "parts": parts}]
            
        # Generate content using the Gemini API
        generate_content_config = types.GenerateContentConfig(response_mime_type="text/plain")

        response_text = ""
        # Use a standard 'for' loop as generate_content_stream returns a sync iterator
        for chunk in client.models.generate_content_stream(
            model=GEMINI_MODEL,
            contents=contents,
            config=generate_content_config,
        ):
            response_text += chunk.text

        return response_text.strip()

    except Exception as e:
        print(f"An error occurred during Gemini multimodal generation: {e}")
        traceback.print_exc()
        # Ensure the original error type isn't lost if it's already an HTTPException
        if isinstance(e, HTTPException):
             raise e
        # Wrap other exceptions in a 503 Service Unavailable
        raise HTTPException(status_code=503, detail=f"Error generating response from Gemini: {e}")

# --- API Endpoints ---

@app.get("/health", summary="Health Check", tags=["Health"])
async def health_check():
    """Simple health check endpoint."""
    # Could add checks for API key presence here if desired
    return {"status": "ok"}

@app.post("/query", response_model=MultimodalResponse)
async def query_endpoint(query: str = Form(...), image: UploadFile = File(None)):
    """Handles form data with query text and optional image file."""
    start_time = time.time()
    
    try:
        # Process the image if provided
        image_data = None
        if image and image.filename:
            contents = await image.read()
            # Here you could process the image with Gemini or another model
            print(f"Received image: {image.filename}, size: {len(contents)} bytes")
        
        # Get query embedding
        query_embedding = await get_jina_embedding_for_query(query)
        
        # Search Pinecone across multiple namespaces
        retrieved_docs = await search_pinecone_multiple_namespaces(
            query_embedding,
            PINECONE_TOP_K_PER_NAMESPACE,
            PINECONE_INDEX_HOST,
            PINECONE_NAMESPACES
        )
        
        # Extract URLs for content from astrollava-embeddings namespace
        content_urls = []
        for doc in retrieved_docs:
            if doc.url:
                content_urls.append(doc.url)
        
        # Generate response
        text_response = await generate_response_with_gemini(query, retrieved_docs)
        
        print(f"Total processing time: {time.time() - start_time:.2f}s")
        
        # Return structured response matching what the frontend expects
        return MultimodalResponse(
            text_response=text_response,
            image_url=None,  # You can process and return image URLs if needed
            content_urls=content_urls if content_urls else None,
            message="Query processed successfully"
        )
    
    except Exception as e:
        print(f"Error processing query: {e}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"An error occurred while processing the query: {str(e)}"
        )

@app.post("/query_multimodal/", response_model=MultimodalResponse, summary="Multimodal Query", tags=["Multimodal"])
async def multimodal_query(
    text: Optional[str] = Form(default=None),
    image: Optional[UploadFile] = File(default=None)
):
    """
    Handles multimodal queries with text and images.
    The endpoint processes the image with Gemini to get a description,
    retrieves relevant contexts from Pinecone using the description,
    and generates a response based on both the image and text inputs.
    """
    start_time = time.time()
    
    try:
        if not text and not image:
            raise HTTPException(status_code=400, detail="Either text or image must be provided")
        
        # Step 1: Process the image with Gemini to get a description if image is provided
        image_description = None
        if image and image.filename:
            try:
                # Read image content
                image_bytes = await image.read()
                print(f"Processing image: {image.filename}, size: {len(image_bytes)} bytes")
                
                # Configure Gemini client
                client = genai.Client(api_key=GEMINI_API_KEY)
                
                # Prepare multimodal prompt for image description
                image_prompt = "Describe this astronomy image in detail. Focus on celestial objects, their features, and any notable characteristics."
                
                # Convert image to base64 for Gemini
                image_base64 = base64.b64encode(image_bytes).decode('utf-8')
                mime_type = image.content_type or "image/jpeg"
                
                # Create multimodal content for Gemini using the dictionary format
                # This is more compatible across different versions of the library
                contents = [
                    {
                        "role": "user",
                        "parts": [
                            {
                                "inline_data": {
                                    "mime_type": mime_type,
                                    "data": image_base64
                                }
                            },
                            {
                                "text": image_prompt
                            }
                        ]
                    }
                ]
                
                # Generate image description using the client.models.generate_content method
                response = client.models.generate_content(
                    model="gemini-2.0-flash",
                    contents=contents
                )
                
                image_description = response.text
                print(f"Generated image description: {image_description}")
                
            except Exception as e:
                print(f"Error processing image with Gemini: {e}")
                traceback.print_exc()
                image_description = "Failed to process image."
        
        # Step 2: Create a combined query from text and image description
        combined_query = ""
        if text:
            combined_query += text
        if image_description:
            if combined_query:
                combined_query += "\n\nImage description: " + image_description
            else:
                combined_query = "Image description: " + image_description
        
        # Step 3: Get embedding for the combined query
        query_embedding = await get_jina_embedding_for_query(combined_query)
        
        # Step 4: Search Pinecone with the embedding
        retrieved_docs = await search_pinecone_multiple_namespaces(
            query_embedding,
            PINECONE_TOP_K_PER_NAMESPACE,
            PINECONE_INDEX_HOST,
            PINECONE_NAMESPACES
        )
        
        # Step 5: Extract URLs for content
        content_urls = [doc.url for doc in retrieved_docs if doc.url]
        
        # Step 6: Generate final response using Gemini with both text and image
        final_prompt = combined_query
        if text:
            final_prompt = f"User query: {text}\n\n{final_prompt}"
            
        # Use the new function that includes the image in the final query
        image_mime_type = image.content_type if image and image.filename else None
        final_response = await generate_response_with_gemini_multimodal(
            query=final_prompt, 
            context_items=retrieved_docs,
            image_bytes=image_bytes if image and image.filename else None,
            mime_type=image_mime_type
        )
        
        print(f"Total multimodal processing time: {time.time() - start_time:.2f}s")
        
        return MultimodalResponse(
            text_response=final_response,
            image_url=None,  # We don't generate images, only process them
            content_urls=content_urls if content_urls else None,
            message="Multimodal query processed successfully"
        )
    
    except Exception as e:
        print(f"Error in multimodal query endpoint: {e}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"An error occurred while processing the multimodal query: {str(e)}"
        )

# --- Run the API with Uvicorn ---
if __name__ == "__main__":
    import uvicorn
    print("--- Starting FastAPI Server ---")
    print("Ensure API keys are set via .env file or environment variables.")
    print(f"Using {len(PINECONE_NAMESPACES)} Pinecone namespaces: {', '.join(PINECONE_NAMESPACES)}")
    print(f"Retrieving top {PINECONE_TOP_K_PER_NAMESPACE} results from each namespace")
    print("API Docs available at http://127.0.0.1:8000/docs")
    # Use reload=True for development, disable in production
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)