import requests
import json
import os
import time
import traceback
from fastapi import FastAPI, HTTPException, Body, Form, UploadFile, File
from pydantic import BaseModel, Field
from dotenv import load_dotenv # To load .env file
from typing import List, Optional, Dict, Any
from fastapi.middleware.cors import CORSMiddleware
import base64
from io import BytesIO

# --- Load Environment Variables ---
load_dotenv()

# --- Configuration ---
# Load from environment or use placeholders (update placeholders if not using .env)
JINA_API_URL = 'https://api.jina.ai/v1/embeddings'
JINA_MODEL = "jina-embeddings-v3"
JINA_API_KEY = os.getenv("JINA_API_KEY", "YOUR_JINA_API_KEY")

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY", "YOUR_PINECONE_API_KEY")
PINECONE_INDEX_HOST = os.getenv("PINECONE_INDEX_HOST", "YOUR_INDEX_HOST")
# <<< IMPORTANT: Verify this namespace matches where your Hubble data is stored >>>
PINECONE_NAMESPACE = "hubble-embeddings"
PINECONE_TOP_K = 5

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "YOUR_GEMINI_API_KEY")
GEMINI_MODEL = "gemini-2.0-flash" # Using Flash for potentially faster responses

# Pinecone Client (Initialize later when needed or globally if preferred)
from pinecone import Pinecone
try:
    # Try importing the specific exception class if available
    from pinecone.exceptions import ApiException
except ImportError:
    # Fallback to general Exception if the specific class isn't found
    print("Warning: Could not import pinecone.exceptions.ApiException. Using general Exception for Pinecone errors.")
    ApiException = Exception # Use general Exception as a fallback

# Gemini Client (Import the correct package)
import google.genai as genai
from google.genai import types

# --- Pydantic Models for API ---
class QueryRequest(BaseModel):
    query: str = Field(..., min_length=1, description="The user's query about astronomy/Hubble.")

class QueryResponse(BaseModel):
    answer: str = Field(..., description="The RAG-generated answer.")
    retrieved_context: List[str] = Field(default=[], description="List of context snippets retrieved from Pinecone.")
    error: Optional[str] = Field(default=None, description="Error message if processing failed.")

# --- Dummy Models for Multimodal ---
class MultimodalQueryRequest(BaseModel):
    text: Optional[str] = Field(default=None, description="Optional text part of the query.")
    image_base64: Optional[str] = Field(default=None, description="Optional base64 encoded image data.")

class MultimodalResponse(BaseModel):
    text_response: Optional[str] = Field(default=None, description="Generated text response.")
    image_url: Optional[str] = Field(default=None, description="URL of a generated image (dummy).")
    message: str = Field(..., description="Status message.")

# --- Initialize FastAPI App ---
app = FastAPI(
    title="Astronomy RAG API",
    description="Provides answers to astronomy questions using Hubble data retrieved via vector search and generated by Gemini.",
    version="1.0.0",
)

origins = ["*"] # Allow all origins

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
# --- Helper Functions (Adapted for API Context) ---

async def get_jina_embedding_for_query(query_text: str) -> Optional[List[float]]:
    """Gets a single embedding vector for the user query using Jina."""
    if not JINA_API_KEY or JINA_API_KEY == "YOUR_JINA_API_KEY":
        print("Error: Jina API key not configured.")
        raise HTTPException(status_code=500, detail="Jina API key not configured.")

    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {JINA_API_KEY}'}
    data = {"model": JINA_MODEL, "input": [query_text]}

    try:
        # Using requests synchronously (FastAPI handles threading)
        response = requests.post(JINA_API_URL, headers=headers, data=json.dumps(data), timeout=60)
        response.raise_for_status()
        response_data = response.json()

        # Check response structure (Jina v3)
        if isinstance(response_data, list) and len(response_data) > 0 and isinstance(response_data[0], dict) and 'embedding' in response_data[0]:
            return response_data[0]['embedding']
        elif isinstance(response_data, dict) and 'data' in response_data and isinstance(response_data['data'], list) and len(response_data['data']) > 0 and isinstance(response_data['data'][0], dict) and 'embedding' in response_data['data'][0]:
             return response_data['data'][0]['embedding']
        else:
            print(f"Error: Unexpected response format from Jina API: {response_data}")
            raise HTTPException(status_code=500, detail="Unexpected response format from Jina embedding API.")

    except requests.exceptions.RequestException as e:
        status_code = 500
        detail = f"Error calling Jina API: {e}"
        if hasattr(e, 'response') and e.response is not None:
            status_code = e.response.status_code
            try: detail = f"Jina API Error (Status {status_code}): {e.response.json()}"
            except json.JSONDecodeError: detail = f"Jina API Error (Status {status_code}): {e.response.text}"
        print(detail)
        # Return 503 Service Unavailable for rate limits, 500 otherwise
        raise HTTPException(status_code=503 if status_code == 429 else 500, detail=detail)
    except Exception as e:
        print(f"An unexpected error occurred during Jina embedding: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Internal server error during embedding: {e}")

async def search_pinecone(query_vector: List[float], top_k: int, index_host: str, namespace: str) -> List[str]:
    """Queries Pinecone index and returns relevant context snippets."""
    if not PINECONE_API_KEY or PINECONE_API_KEY == "YOUR_PINECONE_API_KEY":
        raise HTTPException(status_code=500, detail="Pinecone API key not configured.")
    if not index_host or index_host == "YOUR_INDEX_HOST":
        raise HTTPException(status_code=500, detail="Pinecone index host not configured.")

    retrieved_contexts = []
    try:
        # Initialize connection (consider making pc global or using dependency injection for reuse)
        pc = Pinecone(api_key=PINECONE_API_KEY)
        index = pc.Index(host=index_host)

        query_response = index.query(
            namespace=namespace,
            vector=query_vector,
            top_k=top_k,
            include_metadata=True,
            include_values=False
        )

        for match in query_response.matches:
            metadata = match.metadata
            if metadata:
                # --- Adjust this extraction based on your actual metadata ---
                # Prioritize fields likely containing the core text
                context = metadata.get("caption", metadata.get("combined_text_preview", metadata.get("text", "")))
                if context and isinstance(context, str): # Ensure context is a non-empty string
                    retrieved_contexts.append(context)
                else:
                    print(f"Warning: Pinecone match ID {match.id} missing/invalid context field in metadata: {metadata}")
            else:
                 print(f"Warning: Pinecone match ID {match.id} has no metadata.")

    except ApiException as e: # Catch Pinecone specific or general Exception
        print(f"Error querying Pinecone: {e}")
        # Customize error code based on e if needed (e.g., 404 if index/host wrong?)
        raise HTTPException(status_code=503, detail=f"Pinecone query failed: {e}")
    except Exception as e:
        print(f"An unexpected error occurred during Pinecone search: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Internal server error during vector search: {e}")

    return retrieved_contexts

async def generate_response_with_gemini(query: str, context_list: List[str]) -> str:
    """Generates a response using Gemini based on the query and retrieved context."""
    if not GEMINI_API_KEY or GEMINI_API_KEY == "YOUR_GEMINI_API_KEY":
        raise HTTPException(status_code=500, detail="Gemini API key not configured.")

    try:
        # Configure the Gemini client
        client = genai.Client(api_key=GEMINI_API_KEY)

        # Combine retrieved contexts into a single string block
        context_str = "\n\n".join([f"Context {i+1}: {ctx}" for i, ctx in enumerate(context_list)])
        if not context_list:
            context_str = "No relevant context was found."

        # Create a clear prompt for the RAG task
        prompt = f"""You are a helpful assistant knowledgeable about astronomy, specifically the Hubble Space Telescope findings. Answer the following user query based *only* on the provided context information. If the context does not contain the answer, state that clearly. Do not use any external knowledge.

Context Information:
---
{context_str}
---

User Query: {query}

Answer:"""

        # Prepare the contents for the Gemini API
        contents = [
            types.Content(
                role="user",
                parts=[types.Part.from_text(text=prompt)],
            )
        ]

        # Generate content using the Gemini API
        generate_content_config = types.GenerateContentConfig(response_mime_type="text/plain")

        response_text = ""
        # Use a standard 'for' loop as generate_content_stream returns a sync iterator
        for chunk in client.models.generate_content_stream(
            model=GEMINI_MODEL,
            contents=contents,
            config=generate_content_config,
        ):
            response_text += chunk.text

        return response_text.strip()

    except Exception as e:
        print(f"An error occurred during Gemini generation: {e}")
        traceback.print_exc()
        # Ensure the original error type isn't lost if it's already an HTTPException
        if isinstance(e, HTTPException):
             raise e
        # Wrap other exceptions in a 503 Service Unavailable
        raise HTTPException(status_code=503, detail=f"Error generating response from Gemini: {e}")

# --- API Endpoints ---

@app.get("/health", summary="Health Check", tags=["Health"])
async def health_check():
    """Simple health check endpoint."""
    # Could add checks for API key presence here if desired
    return {"status": "ok"}

@app.post("/query/", response_model=QueryResponse, summary="Query the Astronomy RAG", tags=["RAG"])
async def perform_rag_query(query: str = Form(...), image: UploadFile = File(None)):
    """
    Performs Retrieval-Augmented Generation for an astronomy query.
    Accepts form data with query text and optional image.
    """
    print(f"Received query: {query}")
    start_pipeline_time = time.time()
    query_embedding = None
    retrieved_docs = []
    final_answer = ""

    try:
        # Step 1: Embed Query
        t = time.time()
        query_embedding = await get_jina_embedding_for_query(query)
        print(f"Step 1: Embedding took {time.time() - t:.2f}s")

        # Step 2: Search Pinecone
        t = time.time()
        retrieved_docs = await search_pinecone(
            query_embedding,
            PINECONE_TOP_K,
            PINECONE_INDEX_HOST,
            PINECONE_NAMESPACE
        )
        print(f"Step 2: Pinecone search took {time.time() - t:.2f}s. Found {len(retrieved_docs)} docs.")

        # Step 3: Generate Response
        t = time.time()
        final_answer = await generate_response_with_gemini(query, retrieved_docs)
        print(f"Step 3: Gemini generation took {time.time() - t:.2f}s")

    except HTTPException as e:
        # Re-raise HTTPExceptions raised by helpers
        raise e
    except Exception as e:
        # Catch unexpected errors in the main flow
        print(f"Unexpected error in RAG pipeline: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"An internal server error occurred: {e}")

    print(f"Total pipeline time: {time.time() - start_pipeline_time:.2f}s")
    return QueryResponse(answer=final_answer, retrieved_context=retrieved_docs)

@app.post("/query", response_model=MultimodalResponse)
async def query_endpoint(query: str = Form(...), image: UploadFile = File(None)):
    """Handles form data with query text and optional image file."""
    start_time = time.time()
    
    try:
        # Process the image if provided
        image_data = None
        if image and image.filename:
            contents = await image.read()
            # Here you could process the image with Gemini or another model
            print(f"Received image: {image.filename}, size: {len(contents)} bytes")
        
        # Get query embedding
        query_embedding = await get_jina_embedding_for_query(query)
        
        # Search Pinecone
        retrieved_docs = await search_pinecone(
            query_embedding,
            PINECONE_TOP_K,
            PINECONE_INDEX_HOST,
            PINECONE_NAMESPACE
        )
        
        # Generate response
        text_response = await generate_response_with_gemini(query, retrieved_docs)
        
        print(f"Total processing time: {time.time() - start_time:.2f}s")
        
        # Return structured response matching what the frontend expects
        return MultimodalResponse(
            text_response=text_response,
            image_url=None,  # You can process and return image URLs if needed
            message="Query processed successfully"
        )
    
    except Exception as e:
        print(f"Error processing query: {e}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"An error occurred while processing the query: {str(e)}"
        )

# --- Dummy Multimodal Endpoints ---

@app.post("/query_multimodal/", response_model=MultimodalResponse, summary="[Dummy] Multimodal Query", tags=["Multimodal (Dummy)"])
async def dummy_multimodal_query(request: MultimodalQueryRequest):
    """(Dummy Endpoint) Placeholder for handling multimodal queries."""
    return MultimodalResponse(
        text_response="This is a dummy multimodal text response.",
        image_url="https://example.com/dummy_image.jpg",
        message="Multimodal query received (dummy endpoint)."
    )

# --- Run the API with Uvicorn ---
if __name__ == "__main__":
    import uvicorn
    print("--- Starting FastAPI Server ---")
    print("Ensure API keys are set via .env file or environment variables.")
    print("API Docs available at http://127.0.0.1:8000/docs")
    # Use reload=True for development, disable in production
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)